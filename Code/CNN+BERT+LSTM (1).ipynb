{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrbKsdYmRB86",
        "outputId": "7c835745-2ce0-4c6d-f9a3-7c78feaefa02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3FAexYTaeOs",
        "outputId": "4843584d-740e-4512-bdeb-01d622529d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "YX2iCeyxSmGY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "95dd0bc3-6eab-4d18-b52c-40aa055922be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-13c3d7f30492>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnO_SNnvaXKs",
        "outputId": "ec2a2cef-aa06-4066-937a-87322b61be70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE3JnewZal3V",
        "outputId": "385bc5be-0c71-467a-e712-7dca43db652e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "    stop_words = set(stopwords.words( 'english' ))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "rVZQ-gwvau9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/1000/train_800.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/1000/test_200.csv\")\n"
      ],
      "metadata": {
        "id": "KED1Amj0SogI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[['text','label']].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tcApnTCZa67d",
        "outputId": "ca23dffa-c00d-4adc-bae8-6adf98bbe157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0               Fast, no paper jams, prints clearly       1\n",
              "1          Fast PPM, Easy Set-up, Output is dry, ...      1\n",
              "2                              Hello Hewlett Packard      1\n",
              "3                            Great quality and speed      1\n",
              "4                             Fast, sharp, reliable.      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2dbf0afc-9a9a-48ba-bf96-61bd200c1224\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fast, no paper jams, prints clearly</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fast PPM, Easy Set-up, Output is dry, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hello Hewlett Packard</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Great quality and speed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fast, sharp, reliable.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dbf0afc-9a9a-48ba-bf96-61bd200c1224')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2dbf0afc-9a9a-48ba-bf96-61bd200c1224 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2dbf0afc-9a9a-48ba-bf96-61bd200c1224');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADV0B36pbDRM",
        "outputId": "b577ede0-5b1d-4e6d-fd3e-6bbd5f95301f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    400\n",
              "0    400\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos_sentences = train_df['text'].loc[train_df.label==1]\n",
        "train_neg_sentences = train_df['text'].loc[train_df.label==0]\n",
        "train_pos_sentences = train_pos_sentences.reset_index(drop=True)\n",
        "train_neg_sentences = train_neg_sentences.reset_index(drop=True)\n",
        "\n",
        "test_pos_sentences = test_df['text'].loc[test_df.label==1]\n",
        "test_neg_sentences = test_df['text'].loc[test_df.label==0]\n",
        "test_pos_sentences = test_pos_sentences.reset_index(drop=True)\n",
        "test_neg_sentences = test_neg_sentences.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NGihMqIgbGmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_pos_sentences[0:5].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqqnlCzLbe1k",
        "outputId": "004c0cf8-2e4f-4bd4-dba4-ad0c6539edde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['  Fast, no paper jams, prints clearly '\n",
            " '        Fast PPM, Easy Set-up, Output is dry, great manual'\n",
            " '        Hello Hewlett Packard' '        Great quality and speed'\n",
            " '        Fast, sharp, reliable.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_neg_sentences[0:5].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_d4nW_rbpYd",
        "outputId": "9cc0a755-fdd9-4c3a-a1b6-41cb273286b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['  takes a few moments to change ink cartridges'\n",
            " '        Loud, cartridges have to be cleaned for real quality color'\n",
            " \"        can't print, lousy support, bad communications, never again\"\n",
            " '        Hard to find replacement cartridges'\n",
            " '        As always, expensive inks.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = train_pos_sentences[0]\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VAhFCtKbrR0",
        "outputId": "f3540e27-83d3-4fdd-8b2e-7e068a7e14a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['paper', 'prints', 'clearly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc and add to vocab\n",
        "def add_sent_to_vocab(text,vocab):\n",
        "# clean text\n",
        "    tokens = clean_doc(text)\n",
        "# update counts\n",
        "    vocab.update(tokens)\n",
        "\n",
        "def process_docs_to_vocab(sentList, vocab):\n",
        "    for i in range(len(sentList)):\n",
        "        text = sentList[i]\n",
        "        add_sent_to_vocab(text,vocab)\n",
        "       \n",
        "# turn a doc into clean tokens\n",
        "def clean_doc_wVocab(doc, vocab):\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\t# remove punctuation from each word\n",
        "\ttokens = [re_punc.sub('', w) for w in tokens]\n",
        "\t# filter out tokens not in vocab\n",
        "\ttokens = [w for w in tokens if w in vocab]\n",
        "\ttokens = ' '.join(tokens)\n",
        "\treturn tokens\n",
        "\n",
        "# load all docs in a directory, into tokens\n",
        "def process_docs_to_tokens(sentList, vocab):\n",
        "\tdocuments = list()\n",
        "\t# walk through all sentences\n",
        "\tfor i in range(len(sentList)):\n",
        "\t\tdoc = sentList[i]\n",
        "\t\t# clean doc\n",
        "\t\ttokens = clean_doc_wVocab(doc, vocab)\n",
        "\t\t# add to list\n",
        "\t\tdocuments.append(tokens)\n",
        "\treturn documents\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "    # pad sequences\n",
        "    padded = tf.keras.preprocessing.sequence.pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "    return padded\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "metadata": {
        "id": "iEWCtiZ3bxx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs_to_vocab(train_df['text'], vocab)\n",
        "process_docs_to_vocab(test_df['text'], vocab)\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1L1Am2Tb1VE",
        "outputId": "2526feac-2e7f-4c49-8b73-d7fe3fc58375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278\n",
            "[('easy', 81), ('quality', 59), ('good', 47), ('print', 43), ('great', 42), ('ink', 34), ('use', 31), ('Great', 29), ('fast', 29), ('small', 29), ('paper', 27), ('Easy', 27), ('Good', 27), ('printing', 26), ('price', 25), ('Low', 24), ('Very', 22), ('Not', 22), ('printer', 21), ('No', 20), ('high', 19), ('little', 19), ('cartridges', 19), ('color', 18), ('child', 18), ('expensive', 18), ('hard', 18), ('low', 16), ('seat', 16), ('None', 16), ('slow', 16), ('prints', 15), ('The', 15), ('none', 15), ('well', 14), ('one', 14), ('Fast', 13), ('Excellent', 13), ('older', 13), ('inexpensive', 13), ('stroller', 13), ('Quality', 12), ('cheap', 12), ('It', 12), ('set', 12), ('cartridge', 12), ('Price', 11), ('large', 11), ('takes', 11), ('get', 11)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "\t# load documents\n",
        "\tneg = process_docs_to_tokens(train_neg_sentences, vocab)\n",
        "\tpos = process_docs_to_tokens(train_pos_sentences, vocab)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset_test(vocab):\n",
        "\t# load documents\n",
        "\tneg = process_docs_to_tokens(test_neg_sentences, vocab)\n",
        "\tpos = process_docs_to_tokens(test_pos_sentences, vocab)\n",
        "\tdocs = neg + pos\n",
        "\t# prepare labels\n",
        "\tlabels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "\treturn docs, labels"
      ],
      "metadata": {
        "id": "MyGSm_Dsb5oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APszFmKbb8-8",
        "outputId": "51129bec-ec08-4c83-bb7f-c543da0c9ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 963\n",
            "Maximum length: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['text'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOdgrNHhcETs",
        "outputId": "9e5a9d8b-9513-4f42-959d-8a5c394bdd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fast, no paper jams, prints clearly \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8Hzai_4UcI9U",
        "outputId": "d277048a-aca7-4519-fe9d-a53a9eac0710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'takes moments change ink cartridges'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)"
      ],
      "metadata": {
        "id": "8bYr8INHcLJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Xtrain.shape)\n",
        "print(Xtrain[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHhXY3LicVri",
        "outputId": "b526ec1a-977d-4879-dae0-62b300ea8380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 16)\n",
            "[ 53 458 459  15  60   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = tokenizer\n",
        "print(t.word_counts)\n",
        "#print(t.document_count)\n",
        "#print(t.word_index)\n",
        "#print(t.word_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6i0wKc5cXwR",
        "outputId": "b5f38435-a675-40de-83a5-8bf72399055c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('takes', 12), ('moments', 1), ('change', 1), ('ink', 24), ('cartridges', 11), ('loud', 8), ('cleaned', 1), ('real', 2), ('quality', 86), ('color', 17), ('print', 40), ('lousy', 1), ('support', 11), ('bad', 6), ('never', 1), ('hard', 27), ('find', 4), ('replacement', 4), ('as', 4), ('always', 2), ('expensive', 24), ('inks', 2), ('running', 5), ('costs', 5), ('noisy', 5), ('large', 14), ('problems', 10), ('consumables', 2), ('somewhat', 8), ('no', 20), ('cons', 1), ('come', 1), ('mind', 1), ('still', 4), ('around', 3), ('price', 47), ('constant', 1), ('maintenance', 4), ('pricey', 8), ('cartridge', 6), ('monitor', 1), ('system', 5), ('leaves', 1), ('something', 1), ('not', 21), ('one', 18), ('slow', 13), ('photos', 10), ('medium', 1), ('regular', 2), ('paper', 29), ('hog', 1), ('epson', 1), ('lines', 1), ('output', 15), ('found', 4), ('minor', 4), ('nozzle', 1), ('clogging', 1), ('issue', 2), ('situations', 1), ('blurred', 1), ('picture', 7), ('none', 29), ('use', 54), ('less', 8), ('printer', 20), ('misfeed', 1), ('macintosh', 1), ('localtalk', 1), ('needs', 10), ('optional', 1), ('interface', 2), ('some', 2), ('information', 1), ('electronic', 1), ('version', 1), ('manual', 6), ('unacceptable', 1), ('black', 8), ('text', 7), ('bit', 3), ('shakes', 1), ('printing', 20), ('may', 9), ('shake', 1), ('desktop', 2), ('rendition', 2), ('poor', 19), ('see', 6), ('opinion', 1), ('pretty', 1), ('very', 20), ('design', 12), ('bigger', 2), ('long', 10), ('time', 13), ('warm', 4), ('page', 6), ('high', 22), ('priced', 6), ('great', 67), ('allow', 2), ('stiff', 1), ('copy', 2), ('lack', 3), ('lcd', 4), ('little', 17), ('warmup', 3), ('longer', 1), ('light', 13), ('indicates', 1), ('that', 2), ('tray', 9), ('size', 11), ('nod', 1), ('screen', 2), ('drivers', 7), ('flash', 10), ('focusing', 1), ('ability', 3), ('pictures', 9), ('best', 11), ('your', 3), ('horrible', 2), ('outdoor', 1), ('you', 11), ('get', 14), ('pay', 3), ('low', 25), ('pic', 2), ('hold', 4), ('many', 8), ('pics', 3), ('resoultion', 1), ('resolution', 7), ('xp', 7), ('work', 7), ('eats', 2), ('batteries', 1), ('fast', 70), ('must', 2), ('removed', 1), ('where', 1), ('removable', 2), ('memory', 15), ('ya', 1), ('paid', 1), ('vague', 1), ('instructions', 2), ('arent', 1), ('much', 5), ('even', 2), ('cheap', 32), ('enough', 8), ('better', 7), ('stuff', 1), ('cracker', 1), ('jack', 1), ('boxes', 1), ('understand', 1), ('construction', 2), ('good', 55), ('makes', 5), ('lighting', 1), ('necessary', 2), ('mine', 2), ('vivitar', 1), ('brand', 1), ('name', 3), ('camera', 6), ('puny', 1), ('internal', 3), ('usb', 12), ('powering', 1), ('operate', 2), ('properly', 1), ('worth', 4), ('money', 5), ('built', 4), ('short', 3), ('range', 3), ('battery', 1), ('cf', 1), ('doors', 2), ('grainy', 1), ('broken', 1), ('defective', 2), ('difficult', 8), ('fold', 11), ('new', 2), ('push', 15), ('lying', 1), ('tip', 1), ('travel', 4), ('the', 15), ('bulky', 14), ('malls', 1), ('baby', 12), ('pushing', 3), ('cause', 2), ('topple', 1), ('backwards', 1), ('yet', 7), ('does', 5), ('easily', 12), ('almost', 1), ('big', 7), ('everyday', 1), ('its', 5), ('manipulate', 1), ('take', 5), ('crowds', 1), ('unless', 1), ('disassemble', 1), ('extremely', 8), ('only', 2), ('seat', 19), ('position', 6), ('know', 2), ('folded', 2), ('lock', 1), ('lacking', 1), ('cutesy', 1), ('gadgets', 1), ('others', 1), ('sometimes', 3), ('control', 2), ('cost', 19), ('adjustment', 2), ('getting', 6), ('used', 7), ('jogging', 4), ('stroller', 15), ('turn', 5), ('rear', 4), ('wheel', 2), ('alignment', 1), ('drift', 1), ('too', 9), ('cheaply', 1), ('made', 2), ('head', 8), ('brake', 3), ('set', 8), ('store', 1), ('comfortable', 10), ('toddlers', 1), ('people', 1), ('excuses', 1), ('fit', 4), ('small', 34), ('car', 9), ('rough', 3), ('terrain', 4), ('balance', 2), ('issues', 2), ('wheels', 9), ('kind', 1), ('like', 8), ('tires', 3), ('go', 4), ('flat', 3), ('recline', 4), ('access', 4), ('basket', 17), ('brakes', 3), ('engage', 1), ('first', 4), ('try', 1), ('lacks', 1), ('parking', 2), ('sun', 1), ('shield', 1), ('awkward', 5), ('bottom', 3), ('transport', 2), ('adjustable', 1), ('shopping', 1), ('higher', 1), ('stationary', 1), ('perfect', 5), ('sports', 1), ('hang', 1), ('putting', 1), ('together', 4), ('problem', 4), ('put', 4), ('away', 1), ('can', 6), ('be', 1), ('drawback', 1), ('reclining', 3), ('joggers', 1), ('sitting', 2), ('folding', 3), ('baskets', 1), ('alittle', 1), ('infants', 1), ('wide', 2), ('breaks', 2), ('wear', 2), ('heavy', 14), ('took', 1), ('awhile', 1), ('eight', 1), ('break', 3), ('well', 22), ('every', 3), ('penny', 1), ('need', 5), ('truck', 1), ('it', 12), ('tipped', 1), ('straps', 1), ('caused', 1), ('otherwise', 2), ('turning', 2), ('easy', 90), ('strap', 1), ('legs', 1), ('retracted', 1), ('canopy', 3), ('brushes', 1), ('older', 14), ('childs', 1), ('mesh', 2), ('review', 2), ('product', 5), ('comparisons', 2), ('replace', 2), ('steep', 1), ('attachment', 3), ('bar', 2), ('vehicles', 1), ('unfolding', 1), ('akward', 1), ('cup', 3), ('holder', 2), ('mention', 2), ('double', 4), ('strollers', 6), ('squeaks', 1), ('quite', 6), ('flimsy', 3), ('months', 6), ('lot', 5), ('room', 8), ('back', 7), ('reclined', 4), ('reliability', 2), ('leg', 1), ('missing', 1), ('storage', 9), ('collapse', 3), ('lift', 1), ('accessible', 3), ('engineering', 1), ('front', 6), ('rider', 1), ('gets', 3), ('kicked', 1), ('seats', 7), ('sit', 6), ('straight', 2), ('inaccessible', 2), ('fully', 2), ('squeaky', 1), ('run', 3), ('reach', 3), ('attached', 3), ('trend', 1), ('phone', 2), ('handle', 3), ('height', 1), ('explorer', 1), ('keep', 2), ('my', 3), ('toddler', 8), ('standing', 3), ('when', 1), ('using', 3), ('infant', 10), ('carrier', 2), ('child', 20), ('cannot', 3), ('manuvering', 1), ('stores', 2), ('fairly', 3), ('taller', 1), ('weight', 8), ('manuver', 1), ('distance', 1), ('catapulting', 1), ('occupant', 1), ('due', 1), ('outgrow', 1), ('maneuver', 6), ('hassle', 2), ('feels', 1), ('strong', 2), ('reliable', 11), ('cumbersome', 1), ('flimsiness', 1), ('pops', 1), ('location', 1), ('designed', 3), ('young', 1), ('children', 13), ('lots', 3), ('repair', 1), ('steer', 5), ('harder', 1), ('positions', 2), ('part', 1), ('padding', 1), ('stand', 4), ('normal', 2), ('harness', 1), ('think', 3), ('maneuverability', 4), ('suitable', 1), ('pretoddlers', 1), ('unfold', 2), ('safety', 2), ('concerns', 1), ('restrained', 1), ('probably', 1), ('choice', 1), ('wanderers', 1), ('closed', 1), ('pushed', 1), ('worst', 1), ('ever', 2), ('grow', 1), ('fall', 1), ('bag', 1), ('software', 16), ('compatible', 2), ('scanner', 12), ('scan', 1), ('par', 1), ('scanning', 1), ('linux', 1), ('far', 5), ('customer', 7), ('service', 8), ('inkjet', 2), ('document', 2), ('bed', 3), ('edges', 1), ('copies', 6), ('bend', 1), ('drum', 6), ('unit', 5), ('gave', 1), ('home', 6), ('driver', 4), ('could', 5), ('errors', 1), ('network', 3), ('port', 3), ('cable', 3), ('box', 2), ('documentation', 3), ('cd', 1), ('life', 4), ('obtrusive', 1), ('power', 3), ('consumption', 2), ('setup', 17), ('cardstock', 1), ('crappy', 1), ('postscript', 2), ('upgrade', 1), ('able', 2), ('limited', 2), ('expansion', 1), ('possibilities', 1), ('on', 3), ('board', 1), ('slightly', 2), ('to', 2), ('in', 1), ('web', 1), ('installation', 1), ('seamless', 1), ('automatic', 3), ('envelope', 2), ('feeder', 3), ('fits', 3), ('letter', 2), ('legal', 2), ('stick', 1), ('beyond', 1), ('body', 1), ('machine', 8), ('nonexpandable', 1), ('feeding', 3), ('touch', 1), ('tricky', 3), ('beige', 1), ('crisp', 5), ('inability', 1), ('increase', 1), ('ram', 2), ('hp', 4), ('buggy', 2), ('mac', 4), ('available', 2), ('sticks', 1), ('toner', 11), ('environments', 1), ('envelopes', 3), ('graphics', 6), ('fuzzy', 1), ('square', 1), ('image', 2), ('supplied', 1), ('excellent', 19), ('susceptible', 1), ('jamming', 1), ('glitches', 1), ('everything', 6), ('this', 3), ('shade', 1), ('annoying', 2), ('quick', 5), ('dialog', 1), ('win', 1), ('handling', 1), ('just', 2), ('prints', 15), ('toners', 1), ('costly', 1), ('stock', 1), ('incorrect', 1), ('test', 1), ('clearing', 1), ('jams', 9), ('requires', 1), ('significant', 1), ('component', 1), ('density', 1), ('load', 1), ('piece', 2), ('loader', 1), ('unreliable', 3), ('assembly', 2), ('possible', 1), ('hurt', 1), ('slide', 1), ('tis', 1), ('operating', 3), ('windows', 7), ('users', 1), ('download', 2), ('additional', 2), ('kit', 1), ('burns', 1), ('glossy', 1), ('means', 1), ('restrictions', 1), ('approx', 1), ('pct', 1), ('address', 2), ('graphic', 1), ('jobs', 2), ('nothing', 2), ('really', 6), ('trouble', 1), ('gray', 1), ('background', 1), ('pages', 3), ('smell', 1), ('inconsistent', 1), ('feed', 4), ('continually', 1), ('dimmed', 1), ('lights', 1), ('parallel', 4), ('clear', 5), ('streaking', 2), ('sucky', 1), ('computer', 1), ('figure', 1), ('handset', 1), ('multiple', 3), ('sending', 1), ('zero', 1), ('writer', 1), ('heads', 3), ('book', 1), ('telephone', 2), ('function', 1), ('produces', 1), ('useless', 1), ('fax', 10), ('if', 3), ('have', 2), ('would', 1), ('suggest', 1), ('separate', 3), ('units', 1), ('for', 4), ('flexibility', 1), ('images', 2), ('will', 1), ('items', 1), ('has', 1), ('difficulty', 1), ('occasional', 1), ('cancelling', 1), ('larger', 1), ('offices', 1), ('item', 2), ('arrived', 1), ('damaged', 1), ('having', 1), ('button', 1), ('but', 1), ('got', 1), ('audible', 1), ('answering', 1), ('space', 5), ('thought', 1), ('complicated', 1), ('last', 4), ('half', 1), ('retail', 1), ('simple', 6), ('terrible', 2), ('false', 1), ('printhead', 2), ('fails', 1), ('nonexistent', 1), ('dries', 1), ('marginal', 1), ('deteriorates', 1), ('disgrace', 1), ('capability', 2), ('goes', 3), ('largest', 1), ('video', 1), ('capture', 2), ('lie', 1), ('replaced', 2), ('twice', 1), ('warranty', 6), ('improvement', 1), ('certain', 1), ('fee', 1), ('frequently', 1), ('fail', 1), ('covered', 1), ('photo', 11), ('is', 3), ('compatable', 1), ('obselescence', 1), ('attempt', 1), ('refill', 1), ('oustside', 1), ('reduction', 1), ('drinks', 1), ('repairs', 1), ('now', 1), ('replacing', 1), ('printouts', 2), ('stripes', 1), ('single', 3), ('sheet', 2), ('complete', 1), ('job', 1), ('sized', 1), ('parts', 1), ('performance', 5), ('deficiencies', 1), ('after', 1), ('thousand', 1), ('flatbed', 4), ('off', 1), ('switch', 1), ('brother', 2), ('reputation', 1), ('uses', 1), ('feeds', 1), ('poorly', 1), ('printout', 3), ('clearly', 1), ('ppm', 2), ('hello', 1), ('hewlett', 1), ('packard', 1), ('speed', 14), ('sharp', 4), ('rich', 1), ('super', 4), ('outstanding', 1), ('superior', 1), ('nice', 7), ('overall', 1), ('unbeatable', 1), ('list', 2), ('economy', 1), ('mode', 5), ('pc', 1), ('results', 1), ('customizable', 1), ('settings', 1), ('tone', 1), ('beautiful', 1), ('economical', 2), ('company', 2), ('trust', 1), ('works', 11), ('digital', 4), ('compact', 7), ('east', 1), ('business', 2), ('terrific', 2), ('faster', 1), ('hook', 1), ('install', 5), ('durability', 4), ('ethernet', 1), ('quet', 1), ('ease', 11), ('footprint', 3), ('included', 1), ('electronically', 2), ('there', 1), ('package', 2), ('editing', 2), ('starter', 1), ('inexpensive', 17), ('comes', 2), ('worked', 1), ('affordable', 7), ('wanted', 1), ('kids', 3), ('fun', 3), ('videos', 1), ('sale', 1), ('holds', 3), ('lowest', 2), ('end', 1), ('offer', 1), ('refuse', 1), ('reasonable', 3), ('kid', 1), ('friendly', 1), ('direct', 1), ('tv', 1), ('free', 1), ('average', 1), ('cheeeeeep', 1), ('turns', 2), ('handles', 1), ('lightweight', 22), ('durable', 11), ('want', 1), ('active', 2), ('lifestyle', 1), ('sturdy', 6), ('clean', 1), ('variety', 3), ('walk', 1), ('daughter', 1), ('loves', 1), ('smooth', 10), ('assemble', 3), ('smoooooth', 1), ('ride', 11), ('restraint', 1), ('sunny', 1), ('tripod', 1), ('types', 1), ('rolls', 1), ('virtually', 1), ('looks', 3), ('solidly', 1), ('folds', 4), ('nicely', 1), ('person', 1), ('ideal', 1), ('rides', 1), ('smoothly', 3), ('style', 1), ('comfy', 3), ('awesome', 1), ('taking', 2), ('manuever', 3), ('pull', 1), ('runs', 3), ('stride', 1), ('strolls', 1), ('different', 1), ('choices', 1), ('comfort', 2), ('literally', 1), ('glides', 1), ('convient', 1), ('roller', 1), ('hand', 1), ('tailored', 1), ('shape', 1), ('useful', 3), ('three', 1), ('lifetime', 1), ('lets', 1), ('son', 1), ('look', 1), ('point', 1), ('way', 2), ('differences', 1), ('throw', 1), ('versatile', 1), ('suv', 1), ('absolutely', 1), ('wonderful', 1), ('accommodating', 1), ('twins', 1), ('special', 1), ('maneuvers', 1), ('ration', 1), ('and', 2), ('roomy', 1), ('abuse', 1), ('remains', 1), ('plenty', 1), ('carries', 1), ('two', 2), ('functional', 1), ('ok', 1), ('carseat', 1), ('feature', 4), ('cars', 1), ('converts', 1), ('materials', 1), ('plushy', 1), ('drives', 1), ('clever', 1), ('idea', 6), ('smaller', 2), ('tandem', 1), ('family', 1), ('open', 1), ('newborn', 1), ('constricting', 1), ('offers', 1), ('convenience', 2), ('accommodates', 1), ('meets', 1), ('unique', 2), ('families', 1), ('walks', 1), ('thinks', 1), ('concept', 1), ('versatility', 2), ('preschoolers', 1), ('allows', 3), ('another', 1), ('lifting', 1), ('sits', 1), ('entertainment', 1), ('place', 1), ('seatbelts', 1), ('security', 1), ('freedom', 2), ('seating', 1), ('preschooler', 2), ('attaches', 1), ('drive', 1), ('being', 1), ('chance', 1), ('features', 5), ('resale', 1), ('value', 1), ('hauling', 1), ('aisles', 1), ('options', 1), ('pushes', 1), ('birth', 1), ('copier', 4), ('lasts', 2), ('laser', 7), ('beat', 2), ('sec', 1), ('lasting', 2), ('decent', 3), ('technical', 1), ('attractive', 1), ('day', 1), ('incredible', 1), ('per', 3), ('quiet', 4), ('sleep', 1), ('remarkably', 1), ('incredibly', 1), ('type', 1), ('proper', 1), ('nearly', 2), ('user', 2), ('office', 3), ('upgradrable', 1), ('reasonably', 2), ('expandable', 1), ('tech', 2), ('beats', 1), ('speeds', 1), ('server', 1), ('os', 1), ('relative', 1), ('standby', 1), ('rarely', 1), ('moderate', 1), ('hibernation', 2), ('white', 1), ('hl', 1), ('banding', 1), ('energy', 1), ('saver', 1), ('solution', 1), ('foot', 1), ('right', 1), ('cheaper', 2), ('needed', 1), ('multifunction', 4), ('efficient', 2), ('pcfax', 1), ('of', 1), ('functions', 5), ('switches', 1), ('all', 3), ('faxing', 2), ('companies', 1), ('buttons', 1), ('placed', 1), ('sort', 1), ('copying', 1), ('usage', 1), ('initial', 1), ('send', 1), ('receive', 1), ('muted', 1), ('convenient', 1), ('simplicity', 1), ('scans', 1), ('switching', 1), ('four', 2), ('capacity', 2), ('keeps', 1), ('aftermarket', 1), ('initially', 1), ('mfc', 2), ('moderately', 1), ('multifunctionality', 1), ('functionality', 1), ('key', 1), ('fair', 1), ('usually', 1), ('case', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0c6UrZteh89",
        "outputId": "75b71fba-334a-48b9-8446-3967f1ca4848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "def define_model(length, vocab_size):\n",
        "    # channel 1\n",
        "    inputs1 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding1 = tf.keras.layers.Embedding(vocab_size, 100)(inputs1)\n",
        "    conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "    drop1 = tf.keras.layers.Dropout(0.5)(conv1)\n",
        "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = tf.keras.layers.Flatten()(pool1)\n",
        "    # channel 2\n",
        "    inputs2 = tf.keras.layers.Input(shape=(length,))\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = TFAutoModel.from_pretrained('bert-base-uncased')\n",
        "    input_ids = tf.keras.layers.Input(shape=(length,), dtype=tf.int32, name=\"input_ids\")\n",
        "    bert_embedding = bert_model(input_ids)[0]\n",
        "    pooled_output = bert_embedding[:, 0, :]\n",
        "    dense2 = tf.keras.layers.Dense(100, activation='relu')(pooled_output)\n",
        "    # channel 3\n",
        "    inputs3 = tf.keras.layers.Input(shape=(length,))\n",
        "    embedding3 = tf.keras.layers.Embedding(vocab_size, 100)(inputs3)\n",
        "    lstm3 = tf.keras.layers.LSTM(64)(embedding3)\n",
        "    dense3 = tf.keras.layers.Dense(100, activation='relu')(lstm3)\n",
        "    # merge\n",
        "    merged = tf.keras.layers.concatenate([flat1, dense2, dense3])\n",
        "    # interpretation\n",
        "    dense4 = tf.keras.layers.Dense(10, activation='relu')(merged)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense4)\n",
        "    model = tf.keras.models.Model(inputs=[inputs1, input_ids, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "cDGoy95cefZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)\n",
        "print(max_length)\n",
        "# define model\n",
        "model = define_model( max_length, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2GK1RnqcomF",
        "outputId": "f52fb639-97c6-4053-ee2c-b8b76d97ad39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "963\n",
            "16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 16, 100)      96300       ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 13, 32)       12832       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " input_ids (InputLayer)         [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)           (None, 13, 32)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]']              \n",
            "                                thPoolingAndCrossAt                                               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 16,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 16, 100)      96300       ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 6, 32)        0           ['dropout_74[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_2 (Sl  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 64)           42240       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 192)          0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 100)          76900       ['tf.__operators__.getitem_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 100)          6500        ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 392)          0           ['flatten[0][0]',                \n",
            "                                                                  'dense_5[0][0]',                \n",
            "                                                                  'dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 10)           3930        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 1)            11          ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,817,253\n",
            "Trainable params: 109,817,253\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "model.fit([Xtrain,Xtrain,Xtrain], ytrain, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q90lr9yxcvJ8",
        "outputId": "4cc7417d-e1cf-4492-a992-cc91600c4fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 61s 479ms/step - loss: 0.7430 - accuracy: 0.4988\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 7s 279ms/step - loss: 0.6933 - accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 8s 322ms/step - loss: 0.6932 - accuracy: 0.4900\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 4s 180ms/step - loss: 0.6932 - accuracy: 0.4775\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 5s 193ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - 4s 176ms/step - loss: 0.6932 - accuracy: 0.4725\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 0.6932 - accuracy: 0.4825\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - 3s 128ms/step - loss: 0.6932 - accuracy: 0.5000\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - 3s 130ms/step - loss: 0.6932 - accuracy: 0.4950\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.6932 - accuracy: 0.4837\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - 3s 113ms/step - loss: 0.6931 - accuracy: 0.5000\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - 4s 164ms/step - loss: 0.6932 - accuracy: 0.4712\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - 3s 115ms/step - loss: 0.6932 - accuracy: 0.4938\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - 4s 146ms/step - loss: 0.6932 - accuracy: 0.4775\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.6931 - accuracy: 0.5000\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.6931 - accuracy: 0.5000\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.6915 - accuracy: 0.5063\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - 3s 133ms/step - loss: 0.4756 - accuracy: 0.8350\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - 4s 149ms/step - loss: 0.2641 - accuracy: 0.9187\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.2158 - accuracy: 0.9050\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.1186 - accuracy: 0.9563\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0730 - accuracy: 0.9775\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - 4s 165ms/step - loss: 0.0605 - accuracy: 0.9825\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0449 - accuracy: 0.9850\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0373 - accuracy: 0.9862\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0400 - accuracy: 0.9875\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 0.0402 - accuracy: 0.9862\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0268 - accuracy: 0.9912\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.0265 - accuracy: 0.9900\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 0.0359 - accuracy: 0.9862\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 0.0270 - accuracy: 0.9875\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0282 - accuracy: 0.9887\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 0.0250 - accuracy: 0.9887\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - 4s 176ms/step - loss: 0.0249 - accuracy: 0.9900\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0252 - accuracy: 0.9925\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - 3s 133ms/step - loss: 0.0281 - accuracy: 0.9900\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0240 - accuracy: 0.9925\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0211 - accuracy: 0.9900\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0221 - accuracy: 0.9912\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0361 - accuracy: 0.9862\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0321 - accuracy: 0.9862\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - 3s 121ms/step - loss: 0.0247 - accuracy: 0.9912\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0253 - accuracy: 0.9887\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 0.0270 - accuracy: 0.9900\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0217 - accuracy: 0.9912\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0257 - accuracy: 0.9887\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.0200 - accuracy: 0.9900\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0234 - accuracy: 0.9900\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0222 - accuracy: 0.9887\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 0.0210 - accuracy: 0.9912\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0227 - accuracy: 0.9887\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.0217 - accuracy: 0.9887\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - 3s 134ms/step - loss: 0.0228 - accuracy: 0.9925\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0225 - accuracy: 0.9875\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0265 - accuracy: 0.9912\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0181 - accuracy: 0.9925\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0230 - accuracy: 0.9912\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0223 - accuracy: 0.9912\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0217 - accuracy: 0.9912\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0225 - accuracy: 0.9887\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0259 - accuracy: 0.9887\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.0237 - accuracy: 0.9900\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0195 - accuracy: 0.9912\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0224 - accuracy: 0.9900\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0192 - accuracy: 0.9900\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0230 - accuracy: 0.9912\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0208 - accuracy: 0.9912\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0228 - accuracy: 0.9887\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0177 - accuracy: 0.9912\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0197 - accuracy: 0.9912\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0180 - accuracy: 0.9925\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0263 - accuracy: 0.9887\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0245 - accuracy: 0.9887\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0222 - accuracy: 0.9887\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0222 - accuracy: 0.9900\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0190 - accuracy: 0.9912\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0196 - accuracy: 0.9875\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0252 - accuracy: 0.9862\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - 3s 133ms/step - loss: 0.0140 - accuracy: 0.9937\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0227 - accuracy: 0.9875\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - 4s 120ms/step - loss: 0.0191 - accuracy: 0.9887\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0163 - accuracy: 0.9925\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0257 - accuracy: 0.9925\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0200 - accuracy: 0.9925\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0186 - accuracy: 0.9925\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0185 - accuracy: 0.9937\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0205 - accuracy: 0.9912\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0177 - accuracy: 0.9887\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - 3s 118ms/step - loss: 0.0178 - accuracy: 0.9912\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.0186 - accuracy: 0.9912\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0231 - accuracy: 0.9887\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - 3s 133ms/step - loss: 0.0217 - accuracy: 0.9937\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0183 - accuracy: 0.9900\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - 3s 119ms/step - loss: 0.0198 - accuracy: 0.9912\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - 4s 141ms/step - loss: 0.0196 - accuracy: 0.9925\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0198 - accuracy: 0.9912\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - 3s 116ms/step - loss: 0.0196 - accuracy: 0.9925\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - 3s 117ms/step - loss: 0.0244 - accuracy: 0.9900\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - 3s 120ms/step - loss: 0.0198 - accuracy: 0.9912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd60909640>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_docs, ytest = load_clean_dataset_test(vocab)"
      ],
      "metadata": {
        "id": "sVZAWgocdBkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = encode_docs(tokenizer, max_length, test_docs)"
      ],
      "metadata": {
        "id": "PONDZiTbdE3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate([Xtrain,Xtrain,Xtrain], ytrain, verbose=0)\n",
        "print(' Train Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nob_FVDadHld",
        "outputId": "0935c034-213c-41ed-ba90-a00395a96c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train Accuracy: 99.124998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate([Xtest,Xtest,Xtest], ytest, verbose=0)\n",
        "print( ' Test Accuracy: %f ' % (acc*100))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhPsb9FBdMQ2",
        "outputId": "f439b160-b853-42d3-aac8-6398d7eb3dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test Accuracy: 85.000002 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/gdrive/My Drive/sent_model_multi_input.h5')"
      ],
      "metadata": {
        "id": "hLYq22qzgwpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}