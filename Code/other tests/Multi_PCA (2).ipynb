{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89570,"status":"ok","timestamp":1686469867360,"user":{"displayName":"S.M. Ashraful HASAN","userId":"00424401101905997672"},"user_tz":-360},"id":"aMuX0fdmBFEg","outputId":"fb2efc40-64e5-4171-b0ad-ee54f84fcf44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16920,"status":"ok","timestamp":1686469884274,"user":{"displayName":"S.M. Ashraful HASAN","userId":"00424401101905997672"},"user_tz":-360},"id":"FUuRO2kEBGxg","outputId":"6362392c-51df-4b02-f461-d9bf3e2e0bc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors\u003e=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers) (4.5.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":651},"id":"f80-Sbw2GgL-"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d08a3098548d46b9be27110ea3610cc6","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d7bb8650d8743ceb14c145ad89d467a","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64a224f4d45341ce996f117fe4d9d729","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73bb8fd9f59b43eda14e3ea6ce91f00b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/10:\n","BERT - Train Loss: 0.4812, Train Accuracy: 0.7825\n","BERT - Test Loss: 0.2552, Test Accuracy: 0.9200\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 2/10:\n","BERT - Train Loss: 0.1715, Train Accuracy: 0.9450\n","BERT - Test Loss: 0.2300, Test Accuracy: 0.9300\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 3/10:\n","BERT - Train Loss: 0.0478, Train Accuracy: 0.9900\n","BERT - Test Loss: 0.2810, Test Accuracy: 0.9300\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 4/10:\n","BERT - Train Loss: 0.0115, Train Accuracy: 1.0000\n","BERT - Test Loss: 0.3204, Test Accuracy: 0.9300\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 5/10:\n","BERT - Train Loss: 0.0072, Train Accuracy: 1.0000\n","BERT - Test Loss: 0.2835, Test Accuracy: 0.9400\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 6/10:\n","BERT - Train Loss: 0.0029, Train Accuracy: 1.0000\n","BERT - Test Loss: 0.3013, Test Accuracy: 0.9500\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 7/10:\n","BERT - Train Loss: 0.0041, Train Accuracy: 0.9975\n","BERT - Test Loss: 0.4984, Test Accuracy: 0.9100\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 8/10:\n","BERT - Train Loss: 0.0837, Train Accuracy: 0.9700\n","BERT - Test Loss: 0.2704, Test Accuracy: 0.9300\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 9/10:\n","BERT - Train Loss: 0.0135, Train Accuracy: 0.9950\n","BERT - Test Loss: 0.3511, Test Accuracy: 0.9300\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n","Epoch 10/10:\n","BERT - Train Loss: 0.0055, Train Accuracy: 0.9975\n","BERT - Test Loss: 0.3493, Test Accuracy: 0.9200\n","BERT - Original dimension: 768\n","BERT - Reduced dimension: 128\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from gensim.models import KeyedVectors\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader, Dataset\n","\n","# First Channel - BERT\n","\n","# Define the paths to the train and test data files\n","train_data_path = '/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/500/train_400.csv'\n","test_data_path = '/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/500/test_100.csv'\n","\n","# Define the dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data = pd.read_csv(data_path)\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text = self.data.iloc[idx]['text']\n","        label = self.data.iloc[idx]['label']\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=512,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label)\n","        }\n","\n","# Define the BERT model and training parameters\n","model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","optimizer_bert = torch.optim.AdamW(model_bert.parameters(), lr=2e-5)\n","criterion_bert = torch.nn.CrossEntropyLoss()\n","\n","# Create the data loaders\n","train_dataset_bert = CustomDataset(train_data_path)\n","test_dataset_bert = CustomDataset(test_data_path)\n","train_loader_bert = DataLoader(train_dataset_bert, batch_size=8, shuffle=True)\n","test_loader_bert = DataLoader(test_dataset_bert, batch_size=8)\n","\n","# Training loop for BERT\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_bert.to(device)\n","\n","for epoch in range(10):\n","    train_loss_bert = 0.0\n","    train_acc_bert = 0.0\n","\n","    model_bert.train()\n","    for batch in train_loader_bert:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        optimizer_bert.zero_grad()\n","\n","        outputs = model_bert(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        loss.backward()\n","        optimizer_bert.step()\n","\n","        train_loss_bert += loss.item() * input_ids.size(0)\n","        _, preds = torch.max(logits, dim=1)\n","        train_acc_bert += accuracy_score(labels.cpu(), preds.cpu()) * input_ids.size(0)\n","\n","    train_loss_bert = train_loss_bert / len(train_dataset_bert)\n","    train_acc_bert = train_acc_bert / len(train_dataset_bert)\n","\n","    # Evaluation on the test set\n","    model_bert.eval()\n","    test_loss_bert = 0.0\n","    test_acc_bert = 0.0\n","\n","    with torch.no_grad():\n","        for batch in test_loader_bert:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model_bert(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","            test_loss_bert += loss.item() * input_ids.size(0)\n","            _, preds = torch.max(logits, dim=1)\n","            test_acc_bert += accuracy_score(labels.cpu(), preds.cpu()) * input_ids.size(0)\n","\n","        test_loss_bert = test_loss_bert / len(test_dataset_bert)\n","        test_acc_bert = test_acc_bert / len(test_dataset_bert)\n","\n","    print(f'Epoch {epoch + 1}/{10}:')\n","    print(f'BERT - Train Loss: {train_loss_bert:.4f}, Train Accuracy: {train_acc_bert:.4f}')\n","    print(f'BERT - Test Loss: {test_loss_bert:.4f}, Test Accuracy: {test_acc_bert:.4f}')\n","\n","    # Perform PCA dimensionality reduction on BERT embeddings\n","    train_embeddings_bert = []\n","    for batch in train_loader_bert:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","\n","        with torch.no_grad():\n","            outputs = model_bert.base_model(input_ids, attention_mask=attention_mask)\n","            embeddings = outputs.pooler_output\n","\n","        train_embeddings_bert.append(embeddings.cpu().numpy())\n","\n","    train_embeddings_bert = np.concatenate(train_embeddings_bert)\n","    pca_bert = PCA(n_components=128)\n","    train_embeddings_reduced_bert = pca_bert.fit_transform(train_embeddings_bert)\n","\n","    print(f\"BERT - Original dimension: {train_embeddings_bert.shape[1]}\")\n","    print(f\"BERT - Reduced dimension: {train_embeddings_reduced_bert.shape[1]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AFbsE4YKoLQC"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT - Original dimension embeddings:\n","BERT - Original dimension: 768\n","[[ 0.4666869   0.5344823   0.9346733  ...  0.94740856 -0.1777724\n","   0.64746004]\n"," [-0.01455891 -0.6917085  -0.99928176 ... -0.99966013  0.29300606\n","  -0.75974464]\n"," [ 0.29118216  0.8456919   0.9998205  ...  0.99907     0.23862027\n","   0.66990715]\n"," ...\n"," [-0.35591373 -0.73133457 -0.99921644 ... -0.9996022   0.14060439\n","  -0.554288  ]\n"," [-0.2543459  -0.69907224 -0.99851483 ... -0.999387    0.24491003\n","  -0.59984803]\n"," [ 0.07382118  0.8426459   0.9999542  ...  0.99874455  0.20625168\n","   0.7600613 ]]\n"]}],"source":["print(\"BERT - Original dimension embeddings:\")\n","print(f\"BERT - Original dimension: {train_embeddings_bert.shape[1]}\")\n","print(train_embeddings_bert)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kh_icnH6oV8i"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT - Reduced dimension embeddings:\n","BERT - Reduced dimension: 128\n","[[ 1.54784355e+01 -3.83403182e-01  2.84409904e+00 ...  1.06974645e-02\n","  -3.00777541e-03 -4.85436060e-02]\n"," [-1.83906879e+01 -2.17230940e+00 -4.60091323e-01 ...  1.22108823e-02\n","   3.15596606e-03  8.46820697e-03]\n"," [ 2.14052372e+01 -7.93650150e-01 -1.72815716e+00 ...  2.91166287e-02\n","  -5.22203743e-03  5.04894822e-04]\n"," ...\n"," [-1.90707092e+01  9.66009796e-01 -1.50558669e-02 ... -2.66610663e-02\n","  -3.36460234e-03 -2.52794903e-02]\n"," [-1.88711834e+01  7.73744106e-01 -8.31002831e-01 ... -4.36843745e-02\n","  -6.49864739e-03 -1.97809506e-02]\n"," [ 2.00320206e+01  2.99956465e+00 -3.77757740e+00 ...  1.18456006e-01\n","  -8.60503092e-02 -3.19543406e-02]]\n"]}],"source":["print(\"BERT - Reduced dimension embeddings:\")\n","print(f\"BERT - Reduced dimension: {train_embeddings_reduced_bert.shape[1]}\")\n","print(train_embeddings_reduced_bert)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"46U6R2t5jJNM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10:\n","bi-LSTM - Train Loss: 0.7222, Train Accuracy: 0.5075\n","bi-LSTM - Test Loss: 0.6886, Test Accuracy: 0.5400\n","Epoch 2/10:\n","bi-LSTM - Train Loss: 0.6898, Train Accuracy: 0.5650\n","bi-LSTM - Test Loss: 0.6958, Test Accuracy: 0.5000\n","Epoch 3/10:\n","bi-LSTM - Train Loss: 0.6771, Train Accuracy: 0.5625\n","bi-LSTM - Test Loss: 0.6969, Test Accuracy: 0.5000\n","Epoch 4/10:\n","bi-LSTM - Train Loss: 0.6807, Train Accuracy: 0.5250\n","bi-LSTM - Test Loss: 0.6839, Test Accuracy: 0.5500\n","Epoch 5/10:\n","bi-LSTM - Train Loss: 0.6613, Train Accuracy: 0.5225\n","bi-LSTM - Test Loss: 0.6906, Test Accuracy: 0.5700\n","Epoch 6/10:\n","bi-LSTM - Train Loss: 0.6493, Train Accuracy: 0.5425\n","bi-LSTM - Test Loss: 0.6923, Test Accuracy: 0.6200\n","Epoch 7/10:\n","bi-LSTM - Train Loss: 0.6406, Train Accuracy: 0.5400\n","bi-LSTM - Test Loss: 0.7016, Test Accuracy: 0.5100\n","Epoch 8/10:\n","bi-LSTM - Train Loss: 0.6333, Train Accuracy: 0.5725\n","bi-LSTM - Test Loss: 0.7071, Test Accuracy: 0.6100\n","Epoch 9/10:\n","bi-LSTM - Train Loss: 0.6385, Train Accuracy: 0.5675\n","bi-LSTM - Test Loss: 0.7015, Test Accuracy: 0.4900\n","Epoch 10/10:\n","bi-LSTM - Train Loss: 0.6171, Train Accuracy: 0.5375\n","bi-LSTM - Test Loss: 0.7109, Test Accuracy: 0.6200\n"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn\n","\n","# Define the paths to the train and test data files\n","train_data_path = '/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/500/train_400.csv'\n","test_data_path = '/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/500/test_100.csv'\n","\n","# Define the dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data = pd.read_csv(data_path)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text = self.data.iloc[idx]['text']\n","        label = self.data.iloc[idx]['label']\n","        return {\n","            'text': text,\n","            'label': label\n","        }\n","\n","# Define the bi-LSTM model\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size*2, num_classes)\n","\n","    def forward(self, inputs):\n","        embedded = self.embedding(inputs)\n","        outputs, _ = self.lstm(embedded)\n","        outputs = self.fc(outputs[:, -1, :])\n","        return outputs\n","\n","# Create the data loaders\n","train_dataset_bilstm = CustomDataset(train_data_path)\n","test_dataset_bilstm = CustomDataset(test_data_path)\n","\n","# Prepare vocabulary\n","vocab = set()\n","for data in train_dataset_bilstm:\n","    words = data['text'].split()\n","    vocab.update(words)\n","\n","# Create word-to-index mapping\n","word_to_idx = {word: idx+1 for idx, word in enumerate(vocab)}\n","word_to_idx['\u003cpad\u003e'] = 0\n","\n","# Convert text to numerical sequences\n","def text_to_sequence(text):\n","    words = text.split()\n","    seq = [word_to_idx[word] for word in words if word in word_to_idx]  # Check if word is in vocabulary\n","    return seq\n","\n","# Pad sequences to a fixed length\n","def pad_sequence(seq, max_length):\n","    if len(seq) \u003c max_length:\n","        seq += [word_to_idx['\u003cpad\u003e']] * (max_length - len(seq))\n","    else:\n","        seq = seq[:max_length]\n","    return seq\n","\n","# Define the collate function for data loading\n","def collate_fn(batch):\n","    texts = [data['text'] for data in batch]\n","    labels = [data['label'] for data in batch]\n","    sequences = [text_to_sequence(text) for text in texts]\n","    max_length = max(len(seq) for seq in sequences)\n","    padded_sequences = [pad_sequence(seq, max_length) for seq in sequences]\n","    inputs = torch.LongTensor(padded_sequences)\n","    labels = torch.LongTensor(labels)\n","    return {\n","        'inputs': inputs,\n","        'labels': labels\n","    }\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Set hyperparameters\n","input_size = len(word_to_idx)\n","hidden_size = 128\n","num_classes = 2\n","batch_size = 8\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# Create data loaders\n","train_loader_bilstm = DataLoader(train_dataset_bilstm, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","test_loader_bilstm = DataLoader(test_dataset_bilstm, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","# Create the bi-LSTM model\n","model_bilstm = BiLSTM(input_size, hidden_size, num_classes).to(device)\n","\n","# Define loss function and optimizer\n","criterion_bilstm = nn.CrossEntropyLoss()\n","optimizer_bilstm = torch.optim.Adam(model_bilstm.parameters(), lr=learning_rate)\n","\n","# Training loop for bi-LSTM\n","for epoch in range(num_epochs):\n","    train_loss_bilstm = 0.0\n","    train_acc_bilstm = 0.0\n","\n","    model_bilstm.train()\n","    for batch in train_loader_bilstm:\n","        inputs = batch['inputs'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer_bilstm.zero_grad()\n","\n","        outputs = model_bilstm(inputs)\n","        loss = criterion_bilstm(outputs, labels)\n","\n","        loss.backward()\n","        optimizer_bilstm.step()\n","\n","        train_loss_bilstm += loss.item() * inputs.size(0)\n","        _, preds = torch.max(outputs, dim=1)\n","        train_acc_bilstm += accuracy_score(labels.cpu(), preds.cpu()) * inputs.size(0)\n","\n","    train_loss_bilstm = train_loss_bilstm / len(train_dataset_bilstm)\n","    train_acc_bilstm = train_acc_bilstm / len(train_dataset_bilstm)\n","\n","    # Evaluation on the test set\n","    model_bilstm.eval()\n","    test_loss_bilstm = 0.0\n","    test_acc_bilstm = 0.0\n","\n","    with torch.no_grad():\n","        for batch in test_loader_bilstm:\n","            inputs = batch['inputs'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model_bilstm(inputs)\n","            loss = criterion_bilstm(outputs, labels)\n","\n","            test_loss_bilstm += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, dim=1)\n","            test_acc_bilstm += accuracy_score(labels.cpu(), preds.cpu()) * inputs.size(0)\n","\n","        test_loss_bilstm = test_loss_bilstm / len(test_dataset_bilstm)\n","        test_acc_bilstm = test_acc_bilstm / len(test_dataset_bilstm)\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}:')\n","    print(f'bi-LSTM - Train Loss: {train_loss_bilstm:.4f}, Train Accuracy: {train_acc_bilstm:.4f}')\n","    print(f'bi-LSTM - Test Loss: {test_loss_bilstm:.4f}, Test Accuracy: {test_acc_bilstm:.4f}')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LtyYZtZl0XDT"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-786ea6a55e3e\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 121\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Create the Word2Vec + BiLSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 121\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecBiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# Define loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-7-786ea6a55e3e\u003e\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, num_classes, embeddings)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2VecBiLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 106\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m4.0000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.1000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6.3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 210\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;34m'Embeddings parameter is expected to be 2-dimensional'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"]}],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","import gensim\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn\n","\n","# Define the paths to the train and test data files\n","train_data_path = '/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/500/train_400.csv'\n","test_data_path = '/content/drive/MyDrive/NEEWWWWW/DATA_NEW/PC/500/test_100.csv'\n","word2vec_model_path = '/content/drive/MyDrive/NEEWWWWW/GoogleNews-vectors-negative300.bin'\n","\n","# Define the dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data = pd.read_csv(data_path)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text = self.data.iloc[idx]['text']\n","        label = self.data.iloc[idx]['label']\n","        return {\n","            'text': text,\n","            'label': label\n","        }\n","\n","# Define the bi-LSTM model\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(BiLSTM, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size*2, num_classes)\n","\n","    def forward(self, inputs):\n","        embedded = self.embedding(inputs)\n","        outputs, _ = self.lstm(embedded)\n","        outputs = self.fc(outputs[:, -1, :])\n","        return outputs\n","\n","# Create the data loaders\n","train_dataset = CustomDataset(train_data_path)\n","test_dataset = CustomDataset(test_data_path)\n","\n","# Prepare vocabulary\n","vocab = set()\n","for data in train_dataset:\n","    words = data['text'].split()\n","    vocab.update(words)\n","\n","# Create word-to-index mapping\n","word_to_idx = {word: idx+1 for idx, word in enumerate(vocab)}\n","word_to_idx['\u003cpad\u003e'] = 0\n","\n","# Convert text to numerical sequences\n","def text_to_sequence(text):\n","    words = text.split()\n","    seq = [word_to_idx[word] for word in words if word in word_to_idx]  # Check if word is in vocabulary\n","    return seq\n","\n","# Pad sequences to a fixed length\n","def pad_sequence(seq, max_length):\n","    if len(seq) \u003c max_length:\n","        seq += [word_to_idx['\u003cpad\u003e']] * (max_length - len(seq))\n","    else:\n","        seq = seq[:max_length]\n","    return seq\n","\n","# Define the collate function for data loading\n","def collate_fn(batch):\n","    texts = [data['text'] for data in batch]\n","    labels = [data['label'] for data in batch]\n","    sequences = [text_to_sequence(text) for text in texts]\n","    max_length = max(len(seq) for seq in sequences)\n","    padded_sequences = [pad_sequence(seq, max_length) for seq in sequences]\n","    inputs = torch.LongTensor(padded_sequences)\n","    labels = torch.LongTensor(labels)\n","    return {\n","        'inputs': inputs,\n","        'labels': labels\n","    }\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Set hyperparameters\n","input_size = len(word_to_idx)\n","hidden_size = 300\n","num_classes = 2\n","batch_size = 8\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# Load Word2Vec model\n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)\n","word2vec_embeddings = word2vec_model.vectors\n","\n","# Create the bi-LSTM model with Word2Vec embeddings\n","class Word2VecBiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes, embeddings):\n","        super(Word2VecBiLSTM, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=False)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.fc = nn.Linear(hidden_size*2, num_classes)\n","\n","    def forward(self, inputs):\n","        embedded = self.embedding(inputs)\n","        outputs, _ = self.lstm(embedded)\n","        outputs = self.fc(outputs[:, -1, :])\n","        return outputs\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","# Create the Word2Vec + BiLSTM model\n","model = Word2VecBiLSTM(input_size, hidden_size, num_classes, word2vec_embeddings).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    train_acc = 0.0\n","\n","    model.train()\n","    for batch in train_loader:\n","        inputs = batch['inputs'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * inputs.size(0)\n","        _, preds = torch.max(outputs, dim=1)\n","        train_acc += accuracy_score(labels.cpu(), preds.cpu()) * inputs.size(0)\n","\n","    train_loss = train_loss / len(train_dataset)\n","    train_acc = train_acc / len(train_dataset)\n","\n","    # Evaluation on the test set\n","    model.eval()\n","    test_loss = 0.0\n","    test_acc = 0.0\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            inputs = batch['inputs'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss += loss.item() * inputs.size(0)\n","            _, preds = torch.max(outputs, dim=1)\n","            test_acc += accuracy_score(labels.cpu(), preds.cpu()) * inputs.size(0)\n","\n","        test_loss = test_loss / len(test_dataset)\n","        test_acc = test_acc / len(test_dataset)\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}:')\n","    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}')\n","    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n","\n","# Perform PCA dimensionality reduction on the BiLSTM embeddings\n","train_embeddings = []\n","test_embeddings = []\n","\n","model.eval()\n","with torch.no_grad():\n","    for batch in train_loader:\n","        inputs = batch['inputs'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(inputs)\n","        train_embeddings.append(outputs.cpu().numpy())\n","\n","    for batch in test_loader:\n","        inputs = batch['inputs'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(inputs)\n","        test_embeddings.append(outputs.cpu().numpy())\n","\n","train_embeddings = np.concatenate(train_embeddings)\n","test_embeddings = np.concatenate(test_embeddings)\n","\n","# Perform PCA dimensionality reduction\n","pca = PCA(n_components=2)\n","train_pca = pca.fit_transform(train_embeddings)\n","test_pca = pca.transform(test_embeddings)\n","\n","# Print the shape of the PCA embeddings\n","print(\"Train PCA shape:\", train_pca.shape)\n","print(\"Test PCA shape:\", test_pca.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vSFHk7OIGnnT"},"outputs":[],"source":["# Second Channel - Word2Vec\n","\n","# Define the path to the pretrained Word2Vec model\n","word2vec_model_path = '/content/drive/MyDrive/NEEWWWWW/GoogleNews-vectors-negative300.bin'\n","\n","# Load the Word2Vec model\n","model_word2vec = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)\n","\n","# Define the dataset class for Word2Vec\n","class Word2VecDataset(Dataset):\n","    def __init__(self, data_path):\n","        self.data = pd.read_csv(data_path)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        text = self.data.iloc[idx]['text']\n","        label = self.data.iloc[idx]['label']\n","        return {\n","            'text': text,\n","            'label': label\n","        }\n","\n","# Define the Word2Vec feature extractor\n","def extract_word2vec_features(text):\n","    words = text.split()\n","    feature_vec = np.zeros(300, dtype=np.float32)\n","    n_words = 0\n","    for word in words:\n","        if word in model_word2vec:\n","            feature_vec += model_word2vec[word]\n","            n_words += 1\n","    if n_words \u003e 0:\n","        feature_vec /= n_words\n","    return feature_vec\n","\n","# Define the Word2Vec dataset\n","train_dataset_word2vec = Word2VecDataset(train_data_path)\n","test_dataset_word2vec = Word2VecDataset(test_data_path)\n","\n","# Extract Word2Vec features\n","train_features_word2vec = np.array([extract_word2vec_features(data['text']) for data in train_dataset_word2vec])\n","test_features_word2vec = np.array([extract_word2vec_features(data['text']) for data in test_dataset_word2vec])\n","\n","# Perform PCA dimensionality reduction on Word2Vec features\n","pca_word2vec = PCA(n_components=128)\n","train_features_reduced_word2vec = pca_word2vec.fit_transform(train_features_word2vec)\n","\n","print(f\"Word2Vec - Original dimension: {train_features_word2vec.shape[1]}\")\n","print(f\"Word2Vec - Reduced dimension: {train_features_reduced_word2vec.shape[1]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gcoa9LD2GsdL"},"outputs":[],"source":["# Final Prediction\n","\n","# Check if dimensions match\n","if train_embeddings_reduced_bert.shape[1] != train_features_reduced_word2vec.shape[1]:\n","    raise ValueError(\"Dimensions of reduced features from both channels do not match.\")\n","\n","# Convert reduced features to tensors\n","train_features_reduced_word2vec_tensor = torch.from_numpy(train_features_reduced_word2vec).float().to(device)\n","train_embeddings_reduced_bert_tensor = torch.from_numpy(train_embeddings_reduced_bert).float().to(device)\n","\n","# Define the final classification model\n","model_final = torch.nn.Sequential(\n","    torch.nn.Linear(train_features_reduced_word2vec.shape[1], 64),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(64, 2)\n",").to(device)\n","\n","optimizer_final = torch.optim.Adam(model_final.parameters(), lr=0.001)\n","criterion_final = torch.nn.CrossEntropyLoss()\n","\n","# Convert labels to tensors\n","train_labels_tensor = torch.from_numpy(np.array(train_dataset_bert.data['label'])).long().to(device)\n","\n","# Training loop for final model\n","for epoch in range(10):\n","    optimizer_final.zero_grad()\n","\n","    outputs = model_final(train_features_reduced_word2vec_tensor)\n","    preds = torch.argmax(outputs, dim=1)\n","\n","    loss = criterion_final(outputs, train_labels_tensor)\n","    acc = accuracy_score(train_labels_tensor.cpu(), preds.cpu())\n","\n","    loss.backward()\n","    optimizer_final.step()\n","\n","    print(f'Epoch {epoch + 1}/{10}:')\n","    print(f'Final Model - Train Loss: {loss.item():.4f}, Train Accuracy: {acc:.4f}')\n","\n","# Evaluation on the test set\n","model_final.eval()\n","test_features_reduced_word2vec_tensor = torch.from_numpy(test_features_reduced_word2vec).float().to(device)\n","with torch.no_grad():\n","    test_outputs = model_final(test_features_reduced_word2vec_tensor)\n","    test_preds = torch.argmax(test_outputs, dim=1)\n","    test_loss_final = criterion_final(test_outputs, test_dataset_bert.data['label'].to(device))\n","    test_acc_final = accuracy_score(test_dataset_bert.data['label'], test_preds.cpu())\n","\n","print(f'Final Model - Test Loss: {test_loss_final.item():.4f}, Test Accuracy: {test_acc_final:.4f}')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPl+nJnxsZ6bnJEbWi2E+qi","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06353d7920b6437982d078ae69892859":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f99c74a017a4e248b4680a53dcb9790","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87ed0c28b30147fc93e89af786491158","value":28}},"080c2ef624b846e6a9bfd8c3c58b89cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd57ac75226b4ba48760d55e87bad55a","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1374a85be954be7b6f78ad236a966fa","value":440449768}},"0e60904e3b65416e82ad8b1f063df385":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10f937e0d095425886fbd8367d1b2489":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1c8efabf894575b2d87dbfe365e4c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f99c74a017a4e248b4680a53dcb9790":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23f1a5ef4a8444e197d13e1fec2a9a83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a7cc37ae80c4de9858099bff38ec155":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b1c96698e0f40dc918d32a3a32381c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d72e366f3bd4e7b93035001d3ab64b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"323b7e87197348298452195c81ab998b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df1ed775f22b40efa31722d7de6fb87c","placeholder":"​","style":"IPY_MODEL_a28dcc90184f4398a75618250b6707fe","value":" 570/570 [00:00\u0026lt;00:00, 38.3kB/s]"}},"328c7815f8b14568a13dfa38afb9f13a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a52716baca5449d28b22e7c910b922b6","placeholder":"​","style":"IPY_MODEL_e45078138f1b426f914854bee96bf0b8","value":" 440M/440M [00:01\u0026lt;00:00, 256MB/s]"}},"32deafed65854ef8a91ec1b74f778494":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bb11283d91e47d390c5844ec08e3b42","placeholder":"​","style":"IPY_MODEL_6c9143ccfe31489d93b9ddfd73db976f","value":"Downloading (…)lve/main/config.json: 100%"}},"3379931d1eb940d1adf0ce1e97156ead":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10f937e0d095425886fbd8367d1b2489","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_590797b7016d42ed836e7dab28b518ae","value":231508}},"35b466189484402a8b3d2fb9e99e8436":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d72e366f3bd4e7b93035001d3ab64b4","placeholder":"​","style":"IPY_MODEL_23f1a5ef4a8444e197d13e1fec2a9a83","value":"Downloading model.safetensors: 100%"}},"3a1c1030467a4adab670fb942a07bddf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fd6c74f21b04393829792190aafc506":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4168b2e5e3f64b699fdb70e0883dacec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c1c8efabf894575b2d87dbfe365e4c0","placeholder":"​","style":"IPY_MODEL_673894b9bb3944ad92b26b20c90af621","value":" 232k/232k [00:00\u0026lt;00:00, 2.81MB/s]"}},"4bb11283d91e47d390c5844ec08e3b42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"511637884c85449f950de9330c7d490f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90870caade66472dbdbb239bab790fa7","placeholder":"​","style":"IPY_MODEL_2b1c96698e0f40dc918d32a3a32381c5","value":" 28.0/28.0 [00:00\u0026lt;00:00, 645B/s]"}},"5882fd53e04b4f3cb4a5b9c754ce940a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97a71411a8e54105ab10e478f3900825","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e60904e3b65416e82ad8b1f063df385","value":570}},"590797b7016d42ed836e7dab28b518ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64a224f4d45341ce996f117fe4d9d729":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff81242db7004fc0982985b22adeba7f","IPY_MODEL_3379931d1eb940d1adf0ce1e97156ead","IPY_MODEL_4168b2e5e3f64b699fdb70e0883dacec"],"layout":"IPY_MODEL_973b78b8bd9546c0aef27a415746ff75"}},"673894b9bb3944ad92b26b20c90af621":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c9143ccfe31489d93b9ddfd73db976f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fa2d5827efd4336a734eb1816f57639":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a1c1030467a4adab670fb942a07bddf","placeholder":"​","style":"IPY_MODEL_2a7cc37ae80c4de9858099bff38ec155","value":"Downloading (…)okenizer_config.json: 100%"}},"72f6b295f39a4f78a238e299a1208ea5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73bb8fd9f59b43eda14e3ea6ce91f00b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fa2d5827efd4336a734eb1816f57639","IPY_MODEL_06353d7920b6437982d078ae69892859","IPY_MODEL_511637884c85449f950de9330c7d490f"],"layout":"IPY_MODEL_cd3843297dba475a846457da1886cc3c"}},"87ed0c28b30147fc93e89af786491158":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90870caade66472dbdbb239bab790fa7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"973b78b8bd9546c0aef27a415746ff75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97a71411a8e54105ab10e478f3900825":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d7bb8650d8743ceb14c145ad89d467a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35b466189484402a8b3d2fb9e99e8436","IPY_MODEL_080c2ef624b846e6a9bfd8c3c58b89cc","IPY_MODEL_328c7815f8b14568a13dfa38afb9f13a"],"layout":"IPY_MODEL_cf34ea4c43c74e428dd8640293e2eab3"}},"a28dcc90184f4398a75618250b6707fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a52716baca5449d28b22e7c910b922b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd3843297dba475a846457da1886cc3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf34ea4c43c74e428dd8640293e2eab3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d08a3098548d46b9be27110ea3610cc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32deafed65854ef8a91ec1b74f778494","IPY_MODEL_5882fd53e04b4f3cb4a5b9c754ce940a","IPY_MODEL_323b7e87197348298452195c81ab998b"],"layout":"IPY_MODEL_df70cd7a3b8544f89d257cde1208d8e5"}},"d1374a85be954be7b6f78ad236a966fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd57ac75226b4ba48760d55e87bad55a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df1ed775f22b40efa31722d7de6fb87c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df70cd7a3b8544f89d257cde1208d8e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e45078138f1b426f914854bee96bf0b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff81242db7004fc0982985b22adeba7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72f6b295f39a4f78a238e299a1208ea5","placeholder":"​","style":"IPY_MODEL_3fd6c74f21b04393829792190aafc506","value":"Downloading (…)solve/main/vocab.txt: 100%"}}}}},"nbformat":4,"nbformat_minor":0}