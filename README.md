# Transformer model optimization Bert

In this project, we utilized a pre-trained BERT model to optimize various datasets, including SST-2, Amazon, and COLA. We fine-tuned BERT using
techniques like freezing layers, contextual word embeddings, and incorporating Word2Vec embeddings. These approaches aimed to improve the
model's.

**Technology:** Pytorch, Keras, Tensorflow, Ktrain, Pandas

<table border="0">
  <tr>
    <td><img src="https://drive.google.com/uc?export=view&id=1pCYER9BFdKV5xt80_A8MyGHQOsrc5fR6" alt="GitHub-Mark" width="200"></td>
    <td><img src="https://drive.google.com/uc?export=view&id=1w9NlZdoxwAOvCWY_ab2V0KGWB5Qc-Edb" alt="GitHub-Mark" width="200"></td>
    <td><img src="https://drive.google.com/uc?export=view&id=1W_vy4hGnCbwWrIKN0nRumM_nn3MCgjH8" alt="GitHub-Mark" width="200"></td>
</tr>
  <tr>
    <td><img src="https://drive.google.com/uc?export=view&id=1AKkSxguek7Dnjqncd-85JLec0A9dt-A_" alt="GitHub-Mark" width="200"></td>
    <td><img src="https://drive.google.com/uc?export=view&id=1Nk-IYNFRGokvspLSh_hUVichXV4kZPsd" alt="GitHub-Mark" width="200"></td>
    
    
  </tr>
</table>
