# Transformer model optimization Bert

In this project, we utilized a pre-trained BERT model to optimize various datasets, including SST-2, Amazon, and COLA. We fine-tuned BERT using
techniques like freezing layers, contextual word embeddings, and incorporating Word2Vec embeddings. These approaches aimed to improve the
model's.

**Technology:** Pytorch, Keras, Tensorflow, Ktrain, Pandas
